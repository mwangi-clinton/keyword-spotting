{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b8d9c72-cd32-499d-9358-3708ec94e281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/capstor/scratch/cscs/ckuya\n"
     ]
    }
   ],
   "source": [
    "%cd /capstor/scratch/cscs/ckuya/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2ec26f-d99e-45a3-a8ef-0fe836b1aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import jiwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd8cfb50-1a35-4101-8565-0788d584bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab7c7c-74f9-483a-96bf-6c1e02e31792",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00b639-76d5-400a-8fa4-2636f172276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MelCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # (B, 1, 40, 20) → (B, 16, 40, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                            # (B, 16, 20, 10)\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), # (B, 32, 20, 10)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # (B, 32, 10, 5)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                                # (B, 32*10*5)\n",
    "            nn.Linear(32 * 10 * 5, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class SbuLSTMClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_mels: int            = 40,\n",
    "                 hidden_dim: int        = 128,\n",
    "                 uni_layers: int        = 1,\n",
    "                 num_classes: int       = 30,\n",
    "                 dropout: float         = 0.3):\n",
    "        super(SbuLSTMClassifier, self).__init__()\n",
    "        # 1) Bidirectional LSTM feature extractor\n",
    "        self.bdlstm = nn.LSTM(\n",
    "            input_size   = n_mels,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = 1,\n",
    "            batch_first  = True,\n",
    "            bidirectional= True\n",
    "        )\n",
    "        # 2) Unidirectional LSTM for forward‐only refinement\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size  = 2 * hidden_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers  = uni_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False\n",
    "        )\n",
    "        # 3) MLP classifier on the last time step\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, 1, n_mels, T)\n",
    "        B, C, F, T = x.shape\n",
    "        # remove channel dim → (B, F, T), then time‐first → (B, T, F)\n",
    "        x = x.view(B, F, T).permute(0, 2, 1)\n",
    "\n",
    "        # 1) Bidirectional LSTM → (B, T, 2*hidden_dim)\n",
    "        x, _ = self.bdlstm(x)\n",
    "\n",
    "        # 2) Unidirectional LSTM → (B, T, hidden_dim)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # 3) Take last time step features\n",
    "        last = x[:, -1, :]              # (B, hidden_dim)\n",
    "\n",
    "        # 4) Classify\n",
    "        logits = self.classifier(last)  # (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SbuLSTMFusion(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_mels=40, \n",
    "                 n_mfcc=13, \n",
    "                 hidden_dim=256, \n",
    "                 num_classes=10, \n",
    "                 dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Conv front-end for mel spectrograms\n",
    "        self.conv_mel = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # Pool in frequency domain only\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1))\n",
    "        )\n",
    "        \n",
    "        # Conv front-end for MFCC\n",
    "        self.conv_mfcc = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1,1)),  # No pooling for MFCC (already low-dim)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Calculate output dimensions after convolutions\n",
    "        mel_conv_out = n_mels // 4  # After two /2 max-pooling operations\n",
    "        mfcc_conv_out = n_mfcc     # No reduction in MFCC dimension\n",
    "        \n",
    "        # Deeper BiLSTMs for mel features\n",
    "        self.bdlstm_mel = nn.LSTM(\n",
    "            input_size=64 * mel_conv_out,  # 64 channels × reduced frequency bins\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Deeper BiLSTMs for MFCC features\n",
    "        self.bdlstm_mfcc = nn.LSTM(\n",
    "            input_size=64 * mfcc_conv_out,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for temporal pooling\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=4*hidden_dim,  # 2*hidden_dim from each bidirectional LSTM\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for attention input\n",
    "        self.layer_norm = nn.LayerNorm(4*hidden_dim)\n",
    "        \n",
    "        # Final classifier with improved capacity\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4*hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel, mfcc):\n",
    "        # mel: (B,1,n_mels,T), mfcc: (B,1,n_mfcc,T)\n",
    "        B, _, _, T = mel.shape\n",
    "        \n",
    "        # Apply convolutional front-ends\n",
    "        mel_conv = self.conv_mel(mel)  # (B,64,n_mels/4,T)\n",
    "        mfcc_conv = self.conv_mfcc(mfcc)  # (B,64,n_mfcc,T)\n",
    "        \n",
    "        # Reshape for LSTM: (B,T,C*F)\n",
    "        mel_conv = mel_conv.permute(0, 3, 1, 2)  # (B,T,64,n_mels/4)\n",
    "        mel_conv = mel_conv.reshape(B, T, -1)    # (B,T,64*n_mels/4)\n",
    "        \n",
    "        mfcc_conv = mfcc_conv.permute(0, 3, 1, 2)  # (B,T,64,n_mfcc)\n",
    "        mfcc_conv = mfcc_conv.reshape(B, T, -1)    # (B,T,64*n_mfcc)\n",
    "        \n",
    "        # Apply bidirectional LSTMs\n",
    "        mel_feats, _ = self.bdlstm_mel(mel_conv)   # (B,T,2*hidden_dim)\n",
    "        mfcc_feats, _ = self.bdlstm_mfcc(mfcc_conv)  # (B,T,2*hidden_dim)\n",
    "        \n",
    "        # Concatenate features from both streams\n",
    "        fused_feats = torch.cat([mel_feats, mfcc_feats], dim=2)  # (B,T,4*hidden_dim)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        fused_feats = self.layer_norm(fused_feats)\n",
    "        \n",
    "        # Self-attention for temporal context\n",
    "        attn_out, _ = self.attention(fused_feats, fused_feats, fused_feats)\n",
    "        \n",
    "        # Residual connection\n",
    "        fused_feats = fused_feats + attn_out\n",
    "        \n",
    "        # Global attention-weighted pooling\n",
    "        # Create a learnable query vector for attention-based pooling\n",
    "        query = torch.mean(fused_feats, dim=1, keepdim=True)  # (B,1,4*hidden_dim)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.bmm(query, fused_feats.transpose(1, 2))  # (B,1,T)\n",
    "        attn_weights = F.softmax(attn_scores, dim=2)\n",
    "        \n",
    "        # Apply attention weights to get context vector\n",
    "        context = torch.bmm(attn_weights, fused_feats)  # (B,1,4*hidden_dim)\n",
    "        context = context.squeeze(1)  # (B,4*hidden_dim)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(context)  # (B,num_classes)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1015d56e-70b3-4fb6-8e18-4e8cd2117682",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "48a83d3c-9563-4e0f-adbc-e4d86a8ade59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, root_dir,\n",
    "                 sr=16000,\n",
    "                 n_fft=2048,\n",
    "                 hop_percent=0.75,\n",
    "                 win_length=1600,\n",
    "                 n_mels=40,\n",
    "                 n_mfcc=13,\n",
    "                 max_len=None,\n",
    "                 exclude_folders=None):\n",
    "        \"\"\"\n",
    "        Dataset for evaluation - returns audio features and ground truth labels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root_dir       = root_dir\n",
    "        self.sr             = sr\n",
    "        self.n_fft          = n_fft\n",
    "        self.win_length     = win_length\n",
    "        self.hop_length     = int(win_length * (1-hop_percent))\n",
    "        self.n_mels         = n_mels\n",
    "        self.n_mfcc         = n_mfcc\n",
    "        self.max_len        = max_len\n",
    "        self.exclude_folders= set(exclude_folders or [])\n",
    "\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        # Find all subfolders (classes), filter excludes\n",
    "        labels = sorted([\n",
    "            d for d in os.listdir(self.root_dir)\n",
    "            if os.path.isdir(os.path.join(self.root_dir, d))\n",
    "               and d not in self.exclude_folders\n",
    "        ])\n",
    "        # Build label→index map and reverse mapping for evaluation\n",
    "        self.label_to_idx = {lab: idx for idx, lab in enumerate(labels)}\n",
    "        self.idx_to_label = {idx: lab for lab, idx in self.label_to_idx.items()}\n",
    "\n",
    "        # Walk filesystem and store file paths with labels\n",
    "        self.samples = []\n",
    "        for lab in labels:\n",
    "            folder = os.path.join(self.root_dir, lab)\n",
    "            for fn in os.listdir(folder):\n",
    "                if fn.lower().endswith(('.wav','.mp3')):\n",
    "                    file_path = os.path.join(folder, fn)\n",
    "                    self.samples.append((file_path, self.label_to_idx[lab], lab, fn))\n",
    "\n",
    "        # Sanity check\n",
    "        N = len(self.label_to_idx)\n",
    "        self.samples = [s for s in self.samples if 0 <= s[1] < N]\n",
    "        print(f\"[Evaluation Dataset] Found {len(self.samples)} files \"\n",
    "              f\"under {N} classes: {labels}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _extract_features(self, path):\n",
    "        y, _ = librosa.load(path, sr=self.sr)\n",
    "        # Mel-spectrogram\n",
    "        m = librosa.feature.melspectrogram(\n",
    "            y=y, sr=self.sr,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            n_mels=self.n_mels)\n",
    "        log_mel = librosa.power_to_db(m)\n",
    "        # MFCC\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=y, sr=self.sr,\n",
    "            n_mfcc=self.n_mfcc,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length)\n",
    "        # Normalize\n",
    "        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-8)\n",
    "        mfcc    = (mfcc    - mfcc.mean())    / (mfcc.std()    + 1e-8)\n",
    "        # Pad/truncate in time\n",
    "        if self.max_len is not None:\n",
    "            T = log_mel.shape[1]\n",
    "            if T >= self.max_len:\n",
    "                log_mel = log_mel[:, :self.max_len]\n",
    "                mfcc    = mfcc   [:, :self.max_len]\n",
    "            else:\n",
    "                pw = self.max_len - T\n",
    "                log_mel = np.pad(log_mel, ((0,0),(0,pw)), mode='constant')\n",
    "                mfcc    = np.pad(mfcc,    ((0,0),(0,pw)), mode='constant')\n",
    "        return log_mel, mfcc\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label_idx, label_name, filename = self.samples[idx]\n",
    "        log_mel, mfcc = self._extract_features(path)\n",
    "        return {\n",
    "            'mel':         torch.FloatTensor(log_mel).unsqueeze(0),   # (n_mels, T)\n",
    "            'mfcc':        torch.FloatTensor(mfcc).unsqueeze(0),      # (n_mfcc, T)\n",
    "            'label':       torch.LongTensor([label_idx]).squeeze(),  # For model prediction\n",
    "            'label_name':  label_name,                   # Ground truth word for WER\n",
    "            'filename':    filename,                     # For tracking individual files\n",
    "            'file_path':   path                          # Full path for debugging\n",
    "        }\n",
    "\n",
    "def get_eval_dataloader(root_dir,\n",
    "                       mel_dim     = 40,\n",
    "                       mfcc_dim    = 13,\n",
    "                       max_len     = 20,\n",
    "                       batch_size  = 32,  # Smaller batch size for evaluation\n",
    "                       num_workers = 4,   # Fewer workers for evaluation\n",
    "                       exclude_folders = []):\n",
    "    \"\"\"\n",
    "    Returns evaluation dataloader optimized for WER computation\n",
    "    \n",
    "    Returns: eval_loader, num_classes, label_to_idx, idx_to_label\n",
    "    \"\"\"\n",
    "    ds = AudioFeatureDataset(\n",
    "        root_dir        = root_dir,\n",
    "        n_mels          = mel_dim,\n",
    "        n_mfcc          = mfcc_dim,\n",
    "        max_len         = max_len,\n",
    "        exclude_folders = exclude_folders\n",
    "    )\n",
    "    \n",
    "    num_classes = len(ds.label_to_idx)\n",
    "    \n",
    "    # Evaluation-specific DataLoader settings\n",
    "    eval_loader = DataLoader(\n",
    "        ds, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,           # No shuffling for consistent evaluation\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False          # Include all samples for complete evaluation\n",
    "    )\n",
    "    \n",
    "    return eval_loader, num_classes, ds.label_to_idx, ds.idx_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023dde4f-a8c0-4780-8d3b-44615c4b324e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9147cf1e-0ba6-4fca-8d21-b2f88d22cf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation Dataset] Found 12724 files under 10 classes: ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes']\n"
     ]
    }
   ],
   "source": [
    "eval_loader, num_classes, label_to_idx, idx_to_label = get_eval_dataloader(\n",
    "    root_dir= \"/capstor/scratch/cscs/ckuya/mini_speech_commands\",\n",
    "    batch_size=32,\n",
    "    exclude_folders=['.ipynb_checkpoints']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7f7d0a5e-7cf7-4f01-9653-0c6a8ac51ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_281912/4074467057.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/capstor/scratch/cscs/ckuya/speech-processing/keyword-spotting/source/pipeline/hop50/model9.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model    = SbuLSTMFusion(\n",
    "    n_mels     = 40,\n",
    "    n_mfcc     = 13,\n",
    "    hidden_dim = 128,\n",
    "    num_classes= 10,\n",
    "    dropout    = 0.3\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"/capstor/scratch/cscs/ckuya/speech-processing/keyword-spotting/source/pipeline/hop50/model9.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b5f8d07c-f05b-4d28-9b92-6e06ac0fa8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_wer(model, eval_loader, idx_to_label, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluation function that computes WER\n",
    "    \n",
    "    Args:``\n",
    "        model: Trained model\n",
    "        eval_loader: Evaluation dataloader\n",
    "        idx_to_label: Mapping from class indices to word labels\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        wer: Word Error Rate\n",
    "        predictions: List of predicted words\n",
    "        ground_truths: List of ground truth words\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            # Extract features and move to device\n",
    "            mel_features = batch['mel'].to(device)\n",
    "            mfcc_features = batch['mfcc'].to(device)\n",
    "            logits = model(mel_features, mfcc_features)\n",
    "\n",
    "            pred_indices = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Convert predictions to words\n",
    "            for pred_idx in pred_indices.cpu().numpy():\n",
    "                predictions.append(idx_to_label[pred_idx])\n",
    "            \n",
    "            # Get ground truth words\n",
    "            for label_name in batch['label_name']:\n",
    "                ground_truths.append(label_name)\n",
    "    \n",
    "    \n",
    "    return predictions, ground_truths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8c446f6a-4464-46ca-9a02-d6f9df585e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "predictions, ground_truths = evaluate_with_wer(model, eval_loader, idx_to_label, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d1f68424-6111-4166-bd61-208962b2d0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.4281\n"
     ]
    }
   ],
   "source": [
    "#computing Word Error Rate\n",
    "wer = jiwer.wer(ground_truths, predictions)\n",
    "print(f\"Word Error Rate: {wer:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kws)",
   "language": "python",
   "name": "kws-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
